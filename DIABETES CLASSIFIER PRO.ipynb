{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdb8322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('kaggle_diabetes.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target labels (y)\n",
    "X = df.drop('Outcome', axis=1).values\n",
    "y = df['Outcome'].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4566f595",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, num_iterations=1000):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.weights) + self.bias) >= 1:\n",
    "                    self.weights -= learning_rate * (2 * 1 / num_iterations * self.weights)\n",
    "                else:\n",
    "                    self.weights -= learning_rate * (2 * 1 / num_iterations * self.weights - np.dot(X[i], y[i]))\n",
    "                    self.bias -= learning_rate * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            linear_model = np.dot(sample, self.weights) + self.bias\n",
    "            if linear_model >= 0:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71ea28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 63.25\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit and transform the training set\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set\n",
    "X_test_scaled = sc.transform(X_test)\n",
    "\n",
    "# Instantiate the SVM classifier\n",
    "svm = SVMClassifier()\n",
    "\n",
    "# Fit the SVM classifier\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "svm_predictions = svm.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the accuracy\n",
    "svm_accuracy = round(accuracy_score(y_test, svm_predictions) * 100, 4)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "\n",
    "# Inside the predict_diabetes function\n",
    "def predict_diabetes(Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DPF, Age):\n",
    "    preg = int(Pregnancies)\n",
    "    glucose = float(Glucose)\n",
    "    bp = float(BloodPressure)\n",
    "    st = float(SkinThickness)\n",
    "    insulin = float(Insulin)\n",
    "    bmi = float(BMI)\n",
    "    dpf = float(DPF)\n",
    "    age = int(Age)\n",
    "\n",
    "    x = [[preg, glucose, bp, st, insulin, bmi, dpf, age]]\n",
    "\n",
    "    # Scale the input data\n",
    "    x_scaled = sc.transform(x)\n",
    "\n",
    "    return svm.predict(x_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "495931a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is predicted to have diabetes.\n"
     ]
    }
   ],
   "source": [
    "class SVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.01, num_iterations=1000):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(num_iterations):\n",
    "            for i in range(n_samples):\n",
    "                condition = y[i] * (np.dot(X[i], self.weights) + self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= learning_rate * (2 * 1 / num_iterations * self.weights)\n",
    "                else:\n",
    "                    self.weights -= learning_rate * (2 * 1 / num_iterations * self.weights - np.dot(X[i], y[i]))\n",
    "                    self.bias -= learning_rate * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            linear_model = np.dot(sample, self.weights) + self.bias\n",
    "            if linear_model >= 0:\n",
    "                predictions.append(1)\n",
    "            else:\n",
    "                predictions.append(0)\n",
    "        return np.array(predictions)\n",
    "\n",
    "svm = SVMClassifier()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_predictions = svm.predict(X_test)\n",
    "#svm_accuracy = round(accuracy_score(y_test, svm_predictions), 4) * 100\n",
    "#print(\"SVM Accuracy:\", svm_accuracy)\n",
    "\n",
    "def predict_diabetes(Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DPF, Age):\n",
    "    preg = int(Pregnancies)\n",
    "    glucose = float(Glucose)\n",
    "    bp = float(BloodPressure)\n",
    "    st = float(SkinThickness)\n",
    "    insulin = float(Insulin)\n",
    "    bmi = float(BMI)\n",
    "    dpf = float(DPF)\n",
    "    age = int(Age)\n",
    "\n",
    "    x = [[preg, glucose, bp, st, insulin, bmi, dpf, age]]\n",
    "\n",
    "    prediction = svm.predict(x)\n",
    "\n",
    "    if prediction[0] == 1:\n",
    "        print(\"The patient is predicted to have diabetes.\")\n",
    "    else:\n",
    "        print(\"The patient is predicted not to have diabetes.\")\n",
    "\n",
    "predict_diabetes(5, 120, 92, 10, 81, 26.1, 0.551, 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd1042ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is predicted not to have diabetes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calling the predict_diabetes function\n",
    "prediction = predict_diabetes(2, 81, 72, 15, 76, 30.1, 0.547, 25)\n",
    "\n",
    "# Print the prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"The patient is predicted to have diabetes.\")\n",
    "else:\n",
    "    print(\"The patient is predicted not to have diabetes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39d2fa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You don't have diabetes.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_diabetes(1, 117, 88, 24, 145, 34.5, 0.403, 40)\n",
    "if prediction == 1:\n",
    "  print('Oops! You have diabetes.')\n",
    "else:\n",
    "  print(\"Great! You don't have diabetes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a67e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "def predict_diabetes(Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DPF, Age):\n",
    "    preg = int(Pregnancies)\n",
    "    glucose = float(Glucose)\n",
    "    bp = float(BloodPressure)\n",
    "    st = float(SkinThickness)\n",
    "    insulin = float(Insulin)\n",
    "    bmi = float(BMI)\n",
    "    dpf = float(DPF)\n",
    "    age = int(Age)\n",
    "\n",
    "    x = [[preg, glucose, bp, st, insulin, bmi, dpf, age]]\n",
    "\n",
    "    return svm.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "661f3471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is predicted not to have diabetes.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_diabetes(2, 81, 72, 15, 76, 30.1, 0.547, 25)\n",
    "if prediction == 1:\n",
    "    print(\"The patient is predicted to have diabetes.\")\n",
    "else:\n",
    "    print(\"The patient is predicted not to have diabetes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e0656af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You don't have diabetes.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_diabetes(1, 117, 88, 24, 145, 34.5, 0.403, 40)\n",
    "if prediction == 1:\n",
    "  print('Oops! You have diabetes.')\n",
    "else:\n",
    "  print(\"Great! You don't have diabetes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08a9dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You don't have diabetes.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_diabetes(5, 120, 92, 10, 81, 26.1, 0.551, 67)\n",
    "if prediction:\n",
    "  print('Oops! You have diabetes.')\n",
    "else:\n",
    "  print(\"Great! You don't have diabetes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d770a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient is predicted not to have diabetes.\n"
     ]
    }
   ],
   "source": [
    "prediction = predict_diabetes(2, 120, 70, 30, 80, 25, 0.5, 35)\n",
    "\n",
    "# Print the prediction\n",
    "if prediction == 1:\n",
    "    print(\"The patient is predicted to have diabetes.\")\n",
    "else:\n",
    "    print(\"The patient is predicted not to have diabetes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88195262",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-Nearest Neighbors (KNN) Classifier\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            distances = np.sqrt(np.sum((self.X_train - X[i]) ** 2, axis=1))\n",
    "            nearest_indices = np.argsort(distances)[:self.k]\n",
    "            nearest_labels = self.y_train[nearest_indices]\n",
    "            predictions[i] = np.argmax(np.bincount(nearest_labels))\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "knn = KNNClassifier(k=3)\n",
    "knn.fit(X_train, y_train)\n",
    "knn_predictions = knn.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "print(\"KNN Accuracy:\", knn_accuracy)\n",
    "\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "class LogisticRegressionClassifier:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self._sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self._sigmoid(linear_model)\n",
    "        predictions = [1 if pred > 0.5 else 0 for pred in y_predicted]\n",
    "        return predictions\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "logreg = LogisticRegressionClassifier()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_predictions = logreg.predict(X_test)\n",
    "logreg_accuracy = accuracy_score(y_test, logreg_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logreg_accuracy)\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        classes = np.unique(y)\n",
    "\n",
    "        if len(classes) == 1:\n",
    "            self.tree = classes[0]\n",
    "            return\n",
    "\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            self.tree = np.argmax(np.bincount(y))\n",
    "            return\n",
    "\n",
    "        best_feature, best_split = self._find_best_split(X, y)\n",
    "        left_indices = X[:, best_feature] < best_split\n",
    "        right_indices = ~left_indices\n",
    "\n",
    "        self.tree = {'feature': best_feature, 'split': best_split,\n",
    "                     'left': None, 'right': None}\n",
    "\n",
    "        self.tree['left'] = DecisionTreeClassifier().fit(X[left_indices], y[left_indices], depth + 1)\n",
    "        self.tree['right'] = DecisionTreeClassifier().fit(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(x, self.tree) for x in X])\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gini = 1.0\n",
    "        best_feature = None\n",
    "        best_split = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            values = np.unique(X[:, feature])\n",
    "            for value in values:\n",
    "                left_indices = X[:, feature] < value\n",
    "                right_indices = ~left_indices\n",
    "\n",
    "                gini = self._gini_index(y[left_indices], y[right_indices])\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_feature = feature\n",
    "                    best_split = value\n",
    "\n",
    "        return best_feature, best_split\n",
    "\n",
    "    def _gini_index(self, left_labels, right_labels):\n",
    "        n = len(left_labels) + len(right_labels)\n",
    "        p_left = len(left_labels) / n\n",
    "        p_right = len(right_labels) / n\n",
    "\n",
    "        gini_left = 1.0 - np.sum(np.square(np.bincount(left_labels))) / np.square(len(left_labels))\n",
    "        gini_right = 1.0 - np.sum(np.square(np.bincount(right_labels))) / np.square(len(right_labels))\n",
    "\n",
    "        gini = p_left * gini_left + p_right * gini_right\n",
    "        return gini\n",
    "\n",
    "    def _predict_sample(self, x, tree):\n",
    "        if isinstance(tree, np.integer):\n",
    "            return tree\n",
    "\n",
    "        feature = tree['feature']\n",
    "        split = tree['split']\n",
    "        if x[feature] < split:\n",
    "            return self._predict_sample(x, tree['left'])\n",
    "        else:\n",
    "            return self._predict_sample(x, tree['right'])\n",
    "\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "dt_predictions = dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "\n",
    "\n",
    "# Random Forest Classifier\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, num_trees=100, max_depth=None):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.num_trees):\n",
    "            indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(predictions, axis=0)\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_predictions = rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
